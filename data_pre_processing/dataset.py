import time

import numpy as np
import torch
import xarray as xr
from torch.utils.data import Dataset
from torchvision import transforms
from torchvision.transforms import functional as TF

from data_pre_processing.data_pre_processing_utils import convert_datetime64_array_to_float_tensor


class FilteredDatasetXr(Dataset):
    def __init__(
            self,
            sample_coords,
            radolan_statistics_dict,
            mode,
            settings,
            test_variable_alignment_on_first_batch = True,
            data_into_ram=False,
            baseline_path = None,
            baseline_variable_name = None,
            num_input_frames_baseline = None,
    ):
        """
        The dataset can be in one of three modes:
        'train'     expects padded input (from filtered data) --> random crop augmentations are applied (other augmentastions can be added),
        'baseline'  expects padded input (from filtered data) --> centercrop is applied
        'predict'   expects unpadded input (from unfiltered data)

        Input:
            sample_coords: np.array: Coordinate space
                - generated by patch_indecies_to_sample_coords() -
                array of arrays with valid patch coordinates

                shape: [num_valid_patches, num_dims=3]
                [
                [np.datetime64 target frame,
                slice of y coordinates,
                slice of x coordinates],
                ...]

                if mode == 'train':
                    slice of y coordinates should be s_height_width + input_padding
                if mode == 'baseline':
                    slice of y coordinates should be s_height_width + input_padding
                mode == 'predict'
                    slice of y coordinates should be s_height_width

                x and y coordinates refer to the coordinate system with respect to corred CRS and projection in data_shortened,
                not to lat/lon and also not to the patch coordinates _inner and _outer

            radolan_statistics_dict: dict
                {'mean_filtered_log_data': float, 'std_filtered_log_data': float}
                Make sure this has been only calculated on training data!

            mode: str
                'train':    training mode: _getitem_train_    is called which handles padding and augmentation
                'baseline': baseline mode: _getitem_baseline_ returns the baseline as well, performs center cropping on padded input
                'predict':  evaluation mode: _getitem_eval_   is called which handles non-padded data to do
                                                                operate on unfiltered patches that will be reassembled

        Attributes (most important ones)

            self.dynamic_data_dict
                has the following format: {'name': xr.Dataset, ...} Of all variables that are used for the input
                it includes all data that has a time dimension

            self.static_data_dict
                has the same format {'name': xr.Dataset, ...} and includes static all data that does
                not have a time dimension
                    NOTE: THERE IS NO TRAIN / VAL / TEST SPLIT FOR STATIC DATA as we split along time only

            self.dynamic_variable_name_dict, self.static_variable_name_dict
                {'name': 'variable_name (in xr.Dataset)', ...}
                Mapping from names to variable names in xr dataset

            self.dynamic_statistics_dict, self.static_statistics_dict
                {'name': dict_of_normlaization_statistics}
                i.e. dict_of_normlaization_statistics = {'mean': float, 'std': float}
        """
        # super().__init__()
        # TODO remove this debugging attr
        self.printed=False
        self.sample_coords = sample_coords
        self.num_input_frames_baseline = num_input_frames_baseline
        self.mode = mode
        self.settings = settings

        # Does data have input_padding? This is set by _verify_sample_coord_lengths_()
        self.samples_have_padding = None #  Bool

        s_folder_path = settings['s_folder_path']
        s_dem_path = settings['s_dem_path']
        s_data_file_name = settings['s_data_file_name']

        s_data_variable_name = settings['s_data_variable_name']
        s_dem_variable_name = settings['s_dem_variable_name']
        s_crop_data_time_span = settings['s_crop_data_time_span']
        s_batch_size = settings['s_batch_size']


        # --- load data ---
        # Radolan
        load_path_radolan = '{}/{}'.format(s_folder_path, s_data_file_name)


        if data_into_ram:
            # Load radolan unchunked when reading from RAM
            radolan_data = xr.open_zarr(load_path_radolan, chunks=None, decode_timedelta=False)

        else:
            # Load data chunked when reading from disk
            # radolan_data = xr.open_dataset(load_path_radolan, engine='zarr', chunks=None, decode_timedelta=False)
            # radolan_data = xr.open_dataset(load_path_radolan,
            #                                engine='zarr',
            #                                chunks={'step': 1, 'time': 1, 'y': 1200, 'x': 1100},
            #                                decode_timedelta=False)

            radolan_data = xr.open_zarr(load_path_radolan, chunks=None, decode_timedelta=False)


        # In case only certain time span is used, do some cropping to save RAM
        if s_crop_data_time_span is not None:
            start_time, stop_time = np.datetime64(s_crop_data_time_span[0]), np.datetime64(s_crop_data_time_span[1])
            crop_slice = slice(start_time, stop_time)
            radolan_data = radolan_data.sel(time=crop_slice)

        if data_into_ram:
            radolan_data = radolan_data.load()  # loading into RAM

        # DEM (always loaded into RAM)
        dem_data = xr.open_zarr(s_dem_path, chunks=None)

        if data_into_ram:
            dem_data = dem_data.load() # loading into RAM

        # --- preprocess data ---
        # Radolan
        # Squeeze empty dimension
        radolan_data = radolan_data.squeeze()
        # set all values below 0 to 0

        # TODO Fixme
        # radolan_data = radolan_data.where(radolan_data >= 0, 0)

        #  !!! DONT CALC Z-NORM STATISTICS HERE FOR DYNAMIC DATA AS THIS HAS TO BE DONE ON TRAINING DATA ONLY !!!

        # DEM
        # z normalization, without log norm for DEM!
        # We can do this here on all data as there is no train / val / test split for static data
        dem_mean = float(np.mean(dem_data)[s_dem_variable_name].values)
        dem_std = float(np.std(dem_data)[s_dem_variable_name].values)

        # --- xr data and metadata as dict attributes ---
        # The xarray data as attributes
        self.dynamic_data_dict = {
            'radolan': radolan_data
        }

        # Variable names in xr.Dataset
        self.dynamic_variable_name_dict = {
            'radolan': s_data_variable_name
        }

        # Normalization statistics dicts
        self.dynamic_statistics_dict = {
            'radolan': radolan_statistics_dict
        }

        self.static_data_dict = {
            'dem': dem_data
        }

        self.static_variable_name_dict = {
            'dem': s_dem_variable_name
        }

        self.static_statistics_dict = {
            'dem': {'mean': dem_mean, 'std': dem_std}
        }

        if mode == 'baseline':
            # For dynamic_data_dict['baseline'] special time handling is adjusted to the fact that
            # ! All predictions were assigned to the FIRST INPUT time step !

            # Load baseline data from disk (too large)
            baseline_data = xr.open_zarr(baseline_path)
            self.baseline_data_dict = {'baseline': baseline_data}
            self.baseline_variable_name_dict = {'baseline': baseline_variable_name}
        else:
            # baseline not loaded
            self.baseline_data_dict = {}
            self.baseline_variable_name_dict = {}

        # Check whether the variables (radolan, DEM, ...) are correcly aligned according to their metadata
        # (y, x, lat, lon, time).
        # We do that once during initialization
        # E.g. different coordinate systems etc. can lead to misalignment in latitide and longitude
        self._check_variable_alignment_(num_samples_to_check=s_batch_size)

        # Verify whether sample_coords have been passed according to the mode and sets samples_have_padding accordingly
        # Accepts with padding only for training
        # Accepts with and without padding for prediction
        self._verify_sample_coord_lengths_()

    def __len__(self):
        return len(self.sample_coords)

    def __getitem__(self, idx):
        if self.mode == 'train':
            return self._getitem_train_(idx)

        elif self.mode == 'baseline':
            return self._getitem_baseline_(idx)

        elif self.mode == 'predict':
            return self._getitem_predict_(idx)

        else:
            raise ValueError(f"Invalid mode: self.mode = {self.mode} has to be either 'train' or 'predict'")

    def _getitem_train_(self, idx):
        '''
        This is the getitem method for training / validation.
        The samples are augmented (including random cropping)
        More detailed output information see get_sample_from_coords()
        '''
        sample_coord = self.sample_coords[idx]
        dynamic_samples_dict, static_samples_dict = self.get_sample_from_coords(
            sample_coord,
            load_metadata = False,
            test_metadata_alignment = False
        )
        # We are augmenting here, before batching, so that each individual sample gets its
        # own random augmentation
        dynamic_samples_dict, static_samples_dict = self.augment(dynamic_samples_dict, static_samples_dict)
        return dynamic_samples_dict, static_samples_dict

    def _getitem_baseline_(self, idx):
        '''
        This is the getitem method that also returns the baseline.
        It is assumed that for the baseline
        ! All predictions are assigned to the FIRST INPUT time step !
        The samples are not augmented but center cropped
        More detailed output information see get_sample_from_coords()
        '''

        sample_coord = self.sample_coords[idx]
        dynamic_samples_dict, static_samples_dict, baseline_samples_dict = self.get_sample_from_coords(
            sample_coord,
            load_metadata = False,
            test_metadata_alignment = False
        )
        dynamic_samples_dict, static_samples_dict, baseline_samples_dict = self.center_crop(dynamic_samples_dict,
                                                                                            static_samples_dict,
                                                                                            baseline_samples_dict)
        # dynamic_samples_dict, static_samples_dict, baseline_samples_dict = self.augment(dynamic_samples_dict,
        #                                                                                 static_samples_dict,
        #                                                                                 baseline_samples_dict)
        return dynamic_samples_dict, static_samples_dict, baseline_samples_dict

    def _getitem_predict_(self, idx):
        '''
        This is the getitem method for evaluation, where the metadata is also returned
        Output: (also see get_sample_from_coords())
            dynamic_samples_dict: {'variable_name': timespace chunk that includes input frames and target frame, np.array}
            static_samples_dict: {'variable_name': spacial chunk, np.array}
            sample_coord_float_converted:
                tuple(
                    time: np.timedelta64,
                    y_slice: slice of y coordinates,
                    x_slice slice of x coordinates
                    )
        '''
        sample_coord = self.sample_coords[idx]
        dynamic_samples_dict, static_samples_dict, sample_metadata_dict = self.get_sample_from_coords(
            sample_coord,
            load_metadata=True,
            test_metadata_alignment=False,
        )

        return dynamic_samples_dict, static_samples_dict, sample_metadata_dict

    def _check_variable_alignment_(self, num_samples_to_check):
        """
        This checks whether all variables are aligned according to their metadata
        Includes both, dynamic and static variables.
        ! DOES NOT CHECK BASELINE ALIGNMENT !

        Input:
            num_samples_to_check: int
                The number of random samples that are checked for alignment
        """
        for i in range(num_samples_to_check)    :
            idx = np.random.randint(self.__len__())

            sample_coord = self.sample_coords[idx]
            # This throws errors itself if there is misalignment when test_metadata_alignment=True
            _ = self.get_sample_from_coords(
                sample_coord,
                load_metadata=True,
                test_metadata_alignment=True,
            )

    def get_sample_from_coords(
            self,
            sample_coord: tuple,
            time_step_precipitation_data_minutes=5,
            load_metadata = True,
            test_metadata_alignment = True,

    ):
        '''
        This function takes in the sample coordinates 'sample_coord'.
        Each sample_coord represents one patch / sample.

        The spatial slices in sample_coord have the spatial size of the input
        (optionally + the augmentation padding), which is given by the spatial slices in sample_coord.

        The temporal datetime point gives the time of the target frame (as the filter was applied to the target).
        Therefore, to get the inputs, we have to go back in time relative to the given time in sample_coord
        (depending on lead time and num_input_frames).

        Input:
            sample_coord: tuple(
                time: np.datetime64,
                y_slice: slice of y coordinates, (s_input_height_width + s_padding or s_input_height_width),
                x_slice: slice of x coordinates (s_input_height_width + s_padding or s_input_height_width)
            )
            time_step_precipitation_data_minutes: int, default = 5
                The time step of the precipitation data in minutes.
            load_metadata: Boolean
                If True, metadata is efficiently loaded from the first variable.
            test_metadata_alignment: Boolean
                If True, tests whether all variables have the same metadata (allowing a certain tolerance).
                Slows down the code as metadata for all variables is loaded and compared.

        Output:
            dynamic_variables_dict:
                {'variable_name': timespace chunk that includes input frames and target frame, torch.Tensor}
                Dictionary that includes all 'dynamic' variables (with time dimension).
                Shape: (time, y, x)
                dynamic_variables_dict['radolan'] receives special treatment.

            static_variables_dict:
                {'variable_name': spatial chunk, torch.Tensor}
                Dictionary that includes all 'static' variables (without time dimension).
                Shape: (y, x)

            baseline_variables_dict:
                Only returned if baseline data is loaded and mode == 'baseline'.
                {'baseline': timespace chunk, torch.Tensor}
                Shape: (lead_time, y, x)
                # TODO: IS THIS ASSOCIATED TO RIGHT TIME? CHECK THIS!

            sample_metadata_dict: (only returned if mode == 'predict' or if load_metadata == True)
                {
                    'x':                        np.array, x coordinates,
                    'y':                        np.array, y coordinates,
                    'latitude':                 np.array, latitudes,
                    'longitude':                np.array, longitudes,
                    'time_points_of_spacetime': torch.Tensor converted times
                }

            General info ..._variables_dict:
                - The dataset's __get_item__() method will batch all entries of the dicts and convert them from np.array
                 to torch.Tensor
                - The data is not normalized. All normalization statistics will be calculated

        Depending on the mode:
            - mode == 'train': returns (dynamic_variables_dict, static_variables_dict)
            - mode == 'baseline': returns (dynamic_variables_dict, static_variables_dict, baseline_variables_dict)
            - mode == 'predict': returns (dynamic_variables_dict, static_variables_dict, sample_metadata_dict)

        All normalization/statistics are calculated elsewhere. The data returned here is not normalized.
        '''

        if test_metadata_alignment and not load_metadata:
            raise ValueError(
                'Can only test for metadata alignment, when metadata is loaded. Set "load_metadata" to True')

        num_input_frames = self.settings['s_num_input_time_steps']
        s_num_lead_time_steps = self.settings['s_num_lead_time_steps']

        lead_time = s_num_lead_time_steps
        num_input_frames = int(num_input_frames)
        lead_time = int(lead_time)

        # extract coordinates / coordinate slices
        time_target, y_slice, x_slice = sample_coord

        time_start = (time_target -
                      np.timedelta64(time_step_precipitation_data_minutes * (num_input_frames + lead_time), 'm'))
        time_end = time_target

        time_slice = slice(time_start, time_end)

        dynamic_variables_dict = {}
        sample_metadata_dict = None

        # --- Load dynamic variables ---
        for i, (key, dynamic_data_one_variable) in enumerate(self.dynamic_data_dict.items()):
            spacetime_variable = dynamic_data_one_variable.sel(
                time=time_slice,
                y=y_slice,
                x=x_slice,
            )
            variable_name = self.dynamic_variable_name_dict[key]

            dynamic_variable_values = spacetime_variable[variable_name].values
            dynamic_variable_values = torch.from_numpy(dynamic_variable_values)
            dynamic_variables_dict[key] = dynamic_variable_values

            len_time_slice = np.shape(dynamic_variable_values)[0]
            if not len_time_slice == num_input_frames + lead_time + 1:
                raise ValueError('The time dim of the sample values is not as expected, check the slicing')

            if load_metadata and not test_metadata_alignment:
                if i == 0:
                    # load metadata from first dynamic variable
                    first_variable_metadata_dict = self._load_metadata_from_variable_(spacetime_variable)
                    sample_metadata_dict = first_variable_metadata_dict

            if load_metadata and test_metadata_alignment:
                dynamic_variable_metadata_dict = self._load_metadata_from_variable_(spacetime_variable, mode='dynamic')
                if i == 0:
                    sample_metadata_dict = dynamic_variable_metadata_dict
                else:
                    # comparing metadata of dynamic variables including time
                    is_aligned = self._is_metadata_aligned_(dynamic_variable_metadata_dict,
                                                            previous_variable_metadata_dict,
                                                            compare_time=True)
                    if not is_aligned:
                        raise ValueError('The dynamic variables of one sample are not aligned')
                previous_variable_metadata_dict = dynamic_variable_metadata_dict

        # --- Load static variables ---
        static_variables_dict = {}
        for key, static_variable in self.static_data_dict.items():
            static_variable = static_variable.sel(
                y=y_slice,
                x=x_slice,
            )

            variable_name = self.static_variable_name_dict[key]
            static_variable_values = static_variable[variable_name].values
            static_variable_values = torch.from_numpy(static_variable_values)
            static_variables_dict[key] = static_variable_values

            if load_metadata and test_metadata_alignment:
                static_variable_metadata_dict = self._load_metadata_from_variable_(static_variable, mode='static')
                # comparing static with dynamic metadata (no time)
                is_aligned = self._is_metadata_aligned_(static_variable_metadata_dict,
                                                        previous_variable_metadata_dict,
                                                        compare_time=False)
                if not is_aligned:
                    raise ValueError('The static variables of one sample are not aligned')
                previous_variable_metadata_dict = static_variable_metadata_dict


        # --- Handle baseline if loaded ---
        baseline_variables_dict = {}
        if len(self.baseline_data_dict) > 0:
            # --- LOADING CORRECT TIME OF BASELINE ---
            # --> See Freeform 'load baseline in dataloader' <--
            # The BASELINE is assiciated to its FIRST INPUT FRAME (on time dim),
            # whereas the SPACETIME SAMPLE is associated to the TARGET FRAME
            # We load the baseline from
            # target time
            # - s_num_lead_time_steps   <-- the frame distance between last input frame of model and the target (last entry in spacetime sample)
            #                           <-- Now we are at the position of the last input frame
            # - num_input_frames_baseline <-- Now we are at the first input frame of the baseline

            # The baseline time is associated to its first input frame
            time_start_baseline = (
                    time_target -
                    np.timedelta64(time_step_precipitation_data_minutes * (
                        s_num_lead_time_steps + self.num_input_frames_baseline)
                                   , 'm')
            )

            baseline_data_one_variable = self.baseline_data_dict['baseline']
            baseline_variable_name = self.baseline_variable_name_dict['baseline']
            # select exactly time_start
            # TODO remove the timing debugging thing here
            t0 = time.time()
            if not self.printed:
                print('Baseline sel operation')
            lead_time_delta = np.timedelta64(s_num_lead_time_steps * 5, 'm')

            baseline_spacetime_variable = baseline_data_one_variable.sel(
                time=time_start_baseline,  # we only select the single time = time_start_baseline
                lead_time=lead_time_delta, # Load certain lead time here already ... but lead_time dim seems not to be chunked, so no speedup
                y=y_slice,
                x=x_slice
            )

            baseline_variable_values = baseline_spacetime_variable[baseline_variable_name].values
            if not self.printed:
                print(f"TIME FOR `.sel() + .values` on baseline: {time.time() - t0} seconds")
                self.printed = True
            # baseline is ('lead_time', 'time', 'y', 'x')
            # after selecting single time index = shape (y, x)
            baseline_variable_values = torch.from_numpy(baseline_variable_values)
            baseline_variables_dict['baseline'] = baseline_variable_values

            # No baseline metadata / alignment checking as lat/lon not in there and data was sliced from y / x anyways

        if self.mode == 'train':
            return dynamic_variables_dict, static_variables_dict
        elif self.mode == 'baseline':
            return dynamic_variables_dict, static_variables_dict, baseline_variables_dict
        elif self.mode == 'predict':
            return dynamic_variables_dict, static_variables_dict, sample_metadata_dict

    @staticmethod
    def _load_metadata_from_variable_(spacetime_variable, mode='dynamic'):
        """
        Used in get_sample_from_coords() when load_metadata = True
        This loads metadata from a spacetime_variable

        Input:
            spacetime_variable: np.Array shape: xr.DataSet
                This is the spacetime sample for one variable (like radolan, satellite wavelength, ...)
            mode: str
                can be either 'dynamic' or 'static'
                In case of 'static', time is not loaded
        """
        # Initialize metadata dictionary
        metadata = {}

        # Extract y and add to metadata
        y = spacetime_variable.y.values
        y = torch.from_numpy(y)
        metadata['y'] = y

        # Extract x and add to metadata
        x = spacetime_variable.x.values
        x = torch.from_numpy(x)
        metadata['x'] = x

        # Extract latitude and add to metadata
        lat = spacetime_variable.latitude.values
        lat = torch.from_numpy(lat)
        metadata['latitude'] = lat

        # Extract longitude and add to metadata
        lon = spacetime_variable.longitude.values
        lon = torch.from_numpy(lon)
        metadata['longitude'] = lon

        # If mode is 'dynamic', extract time and add to metadata
        if mode == 'dynamic':
            # Convert time to float tensor, such that it can be passed through dataloader
            time_points_of_spacetime = spacetime_variable.time.values
            time_points_of_spacetime = convert_datetime64_array_to_float_tensor(time_points_of_spacetime)
            metadata['time_points_of_spacetime'] = time_points_of_spacetime

        return metadata

    @staticmethod
    def _is_metadata_aligned_(
            sample_metadata_dict_1,
            sample_metadata_dict_2,
            compare_time=True,
            spatial_tolerance=1e-2,  # Approximately corresponds to 1 km error
            temporal_tolerance_in_seconds=40,
    ):
        """
        This takes in two sample_metadata_dicts and checks whether the metadata is the same (with small
        tolerance). Used in get_sample_from_coords()

        Input:
            sample_metadata_dict_1 / sample_metadata_dict_2: Dictionary
                {
                    'y': y_values,
                    'x': x_values,
                    'latitude': lat_values,
                    'longitude': lon_values,
                    'time_points_of_spacetime': time_values,  # Only required if compare_time=True
                                                                # This is float64 (converted from np.datetime64)
                }

            compare_time: bool
                If True, 'time_points_of_spacetime' is also checked.
        """

        # Compare 'latitude'
        lat1 = sample_metadata_dict_1['latitude']
        lat2 = sample_metadata_dict_2['latitude']
        if not torch.allclose(lat1, lat2, atol=spatial_tolerance):
            raise ValueError('Latitude values do not match between the two samples.')

        # Compare 'longitude'
        lon1 = sample_metadata_dict_1['longitude']
        lon2 = sample_metadata_dict_2['longitude']
        if not torch.allclose(lon1, lon2, atol=spatial_tolerance):
            raise ValueError('Longitude values do not match between the two samples.')

        # Compare 'x' coordinates
        x1 = sample_metadata_dict_1['x']
        x2 = sample_metadata_dict_2['x']
        if not torch.allclose(x1, x2, atol=spatial_tolerance):
            raise ValueError('x coordinates do not match between the two samples.')

        # Compare 'y' coordinates
        y1 = sample_metadata_dict_1['y']
        y2 = sample_metadata_dict_2['y']
        if not torch.allclose(y1, y2, atol=spatial_tolerance):
            raise ValueError('y coordinates do not match between the two samples.')

        # Optionally compare 'time_points_of_spacetime'
        if compare_time:
            time1 = sample_metadata_dict_1['time_points_of_spacetime']
            time2 = sample_metadata_dict_2['time_points_of_spacetime']

            # Convert 40 seconds to nanoseconds since time is in datetime64[ns] converted to float64
            temporal_tolerance = temporal_tolerance_in_seconds * 1e9  # 40 seconds in nanoseconds

            if not torch.allclose(time1, time2, atol=temporal_tolerance):
                raise ValueError('Time points do not match between the two samples within 40 seconds tolerance.')

        # If all comparisons pass, return True
        return True

    def _verify_sample_coord_lengths_(self):
        """
        Verify sample coordinate lengths based on the current mode.

        - In 'train' mode: Only accept padded data.
        - In 'baseline' mode: Only accept padded data.
        - In 'predict' mode: Only accept non-padded data.

        Sets the attribute `self.samples_have_padding` accordingly.

        Raises:
            ValueError: If `sample_coords` is empty or if the slice lengths do not match
                        the expectations based on the mode.
        """
        if len(self.sample_coords) == 0:
            raise ValueError("sample_coords is empty. Cannot perform verification of sample coordinate lengths.")

        # Get a sample coordinate to test
        _, y_slice, x_slice = self.sample_coords[0]

        # Compute the length of the slices in terms of number of elements
        y_coords = self.dynamic_data_dict['radolan'].y.sel(y=y_slice)
        x_coords = self.dynamic_data_dict['radolan'].x.sel(x=x_slice)

        y_length = y_coords.size
        x_length = x_coords.size

        s_input_height_width = self.settings['s_input_height_width']
        s_input_padding = self.settings['s_input_padding']

        if self.mode in ['train', 'baseline']:
            # Require padded data in 'train' and 'baseline' modes
            expected_length = s_input_height_width + s_input_padding
            if y_length != expected_length or x_length != expected_length:
                raise ValueError(
                    f"In '{self.mode}' mode: slice lengths must be "
                    f"s_input_height_width + s_input_padding = {expected_length}. "
                    f"Got y_length={y_length}, x_length={x_length}."
                )
            self.samples_have_padding = True

        elif self.mode == 'predict':
            # Require no padding in 'predict' mode
            expected_length = s_input_height_width
            if y_length != expected_length or x_length != expected_length:
                raise ValueError(
                    f"In 'predict' mode: slice lengths must be exactly "
                    f"s_input_height_width = {expected_length}. "
                    f"Got y_length={y_length}, x_length={x_length}."
                )
            self.samples_have_padding = False

        else:
            raise ValueError(f"Invalid mode: {self.mode}. Must be 'train', 'baseline', or 'predict'.")

    def center_crop(self, *sample_dicts):
        """
        Apply the same center crop to multiple sample dictionaries.

        Input:
            *sample_dicts (dict): Variable number of dictionaries containing image tensors.

        Outpput:
            tuple: Cropped dictionaries in the order they were provided.

        Example:
            cropped_dynamic, cropped_static = self.center_crop(dynamic_samples_dict, static_samples_dict)
        """
        s_input_height_width = self.settings['s_input_height_width']

        # Apply center crop to each dict passed
        cropped_dicts = []
        for sample_dict in sample_dicts:
            cropped_dict = {
                key: TF.center_crop(sample, (s_input_height_width, s_input_height_width))
                for key, sample in sample_dict.items()
            }
            cropped_dicts.append(cropped_dict)

        # Return a tuple of cropped dicts
        return tuple(cropped_dicts)


    def random_crop(self, *sample_dicts):
        """
        Apply the same random crop to multiple sample dictionaries.

        Input:
            *sample_dicts (dict): Variable number of dictionaries containing image tensors.

        Output:
            tuple: Cropped dictionaries in the order they were provided.

        Example:
            cropped_dynamic, cropped_static = self.random_crop(dynamic_samples_dict, static_samples_dict)
        """
        s_input_height_width = self.settings['s_input_height_width']

        if not sample_dicts:
            raise ValueError("At least one sample dictionary must be provided.")

        # Use the first image from the first dictionary as reference
        ref_image = next(iter(sample_dicts[0].values()))
        i, j, h, w = transforms.RandomCrop.get_params(
            ref_image,
            output_size=(s_input_height_width, s_input_height_width)
        )

        cropped_dicts = tuple(
            {key: TF.crop(sample, i, j, h, w) for key, sample in sample_dict.items()}
            for sample_dict in sample_dicts
        )

        return cropped_dicts

    def augment(self, *sample_dicts):
        """
        Apply augmentations to multiple sample dictionaries.

        Currently performs a random crop on all provided dictionaries.

        Args:
            *sample_dicts (dict): Variable number of dictionaries containing image tensors.

        Returns:
            tuple: Augmented (cropped) dictionaries in the order they were provided.

        Example:
            augmented_dynamic, augmented_static = self.augment(dynamic_samples_dict,  )
        """
        cropped_dicts = self.random_crop(*sample_dicts)
        return cropped_dicts
