import random

import pytorch_lightning as pl
from torch.utils.data import Subset, DataLoader

from evaluation.eval_with_baseline import EvaluateBaselineCallback
from helper.checkpoint_handling import get_checkpoint_names, load_from_checkpoint
from helper.memory_logging import format_duration
from load_data_xarray import FilteredDatasetXr
from plotting.plotting_pipeline import plot_logs_pipeline
from evaluation.eval_with_baseline_fss import FSSEvaluationCallback
import time


def evaluation_pipeline(
        data_set_vars, ckpt_settings,
        plot_training_logs=False
):
    '''
    This Pipeline is executed right after training or in 'plotting_only' mode
    '''

    (train_data_loader, validation_data_loader,
     training_steps_per_epoch, validation_steps_per_epoch,
     train_time_keys, val_time_keys, test_time_keys,
     train_sample_coords, val_sample_coords,
     radolan_statistics_dict,
     linspace_binning_params,) = data_set_vars

    # --- Get model checkpoint ---

    save_dir = ckpt_settings['s_dirs']['save_dir']

    all_checkpoint_names = get_checkpoint_names(save_dir)

    # Only do prediction for last checkpoint
    # TODO LOADING 'last' not 'best'
    checkpoints_to_evaluate = []
    checkpoint_name_1 = [name for name in all_checkpoint_names if 'best' in name][0]
    checkpoints_to_evaluate.append(checkpoint_name_1)

    # checkpoint_name_2 = [name for name in all_checkpoint_names if 'last' in name][0]
    # checkpoints_to_evaluate.append(checkpoint_name_2)

    datasets = {
        # 'train': train_data_loader.dataset,
        'val': validation_data_loader.dataset,
    }

    for checkpoint_name in checkpoints_to_evaluate:
        for dataset_name, dataset in datasets.items():
            print(f"\n STARTING EVALUATION ON BASELINE AND MODEL \n ...")
            print(f'\n Checkpoint: {checkpoint_name}, Dataset: {dataset_name} \n')


            model = load_from_checkpoint(
                save_dir,
                checkpoint_name,

                ckpt_settings,
                **ckpt_settings,
            )

            # --- Plot logs ---
            if plot_training_logs:
                plot_logs_pipeline(
                    training_steps_per_epoch,
                    model,
                    ckpt_settings, **ckpt_settings
                )

            # --- Quick evaluation and comparison to baseline over data set ---
            step_start_time = time.time()

            ckpt_quick_eval_with_baseline(
                model,
                checkpoint_name,
                dataset,
                dataset_name,
                radolan_statistics_dict,
                linspace_binning_params,

                ckpt_settings,
                **ckpt_settings
            )
            print(f'\n DONE. Took {format_duration(time.time() - step_start_time)} \n')

        # TODO: Not enabled atm
        # # --- Generate predictions that are saved to a zarr ---
        # print(f"\n STARTING PREDICTIONS AND SAVING TO ZARR \n ...")
        # step_start_time = time.time()
        # ckpt_to_pred(
        #     model,
        #     checkpoint_name,
        #     train_time_keys, val_time_keys, test_time_keys,
        #     radolan_statistics_dict,
        #     linspace_binning_params,
        #     max_num_frames_per_split=3,
        #
        #     splits_to_predict_on=['val'],
        #     ckp_settings=ckpt_settings,
        #     **ckpt_settings,
        # )
        # print(f'\n DONE. Took {format_duration(time.time() - step_start_time)} \n')


def ckpt_quick_eval_with_baseline(
        model,
        checkpoint_name,
        dataset,
        dataset_name,
        radolan_statistics_dict,
        linspace_binning_params,

        ckpt_settings,  # Make sure to pass the settings of the checkpoint
        s_batch_size,
        s_baseline_path,
        s_num_workers_data_loader,
        s_subsample_dataset_to_len,
        s_fss,
        s_fss_scales,
        s_fss_thresholds,
        **__,
):
    """
    Input:
        model:
            Model to evaluate, which has been loaded from checkpoint
        checkpoint_name: str
            The name of the checkpoint from which the model has been loded ... used later for naming the saved files
        sample_coords: np.array: Coordinate space

            - generated by patch_indecies_to_sample_coords() -
            array of arrays with valid patch coordinates

            shape: [num_valid_patches, num_dims=3]
            [
            [np.datetime64 target frame,
            slice of y coordinates,
            slice of x coordinates],
            ...]
    """
    print(f'Baseline path is {s_baseline_path}')
    print('Set model mode')
    # Setting model to baseline mode, which chooses the right predict_step() method
    model.set_mode(mode='baseline')


    print('Initialize Dataset')

    # Setting dataset attributes according to our needs
    # dataset.mode = 'baseline'
    # dataset.data_into_ram = True
    # dataset.settings = ckpt_settings
    # dataset.baseline_path = ckpt_settings['s_baseline_path']
    # dataset.baseline_variable_name = ckpt_settings['s_baseline_variable_name']
    # dataset.num_input_frames_baseline = ckpt_settings['s_num_input_frames_baseline']

     # Data Set
    # We have to initialize the dataset in baseline mode in order for baseline to work
    sample_coords = dataset.sample_coords
    dataset = FilteredDatasetXr(
        sample_coords,
        radolan_statistics_dict,
        mode='baseline',
        settings=ckpt_settings,
        data_into_ram=False,
        baseline_path=ckpt_settings['s_baseline_path'],
        baseline_variable_name=ckpt_settings['s_baseline_variable_name'],
        num_input_frames_baseline=ckpt_settings['s_num_input_frames_baseline'],
    )

    print('Load "samples_have_padding"')
    # Boolean stating whether samples have input padding:F
    # If they do have padding, this is going to be removed by center cropping
    samples_have_padding = dataset.samples_have_padding

    # Subsampling
    sub_sampled = False
    if s_subsample_dataset_to_len is not None:
        if s_subsample_dataset_to_len < len(dataset):
            print(f'Randomly subsample Dataset from length {len(dataset)} to len {s_subsample_dataset_to_len}')
            # Randomly subsample dataset
            subset_indices = random.sample(range(len(dataset)), s_subsample_dataset_to_len)
            # subset_indices = list(range(crop_dataset_to_len))  # Choose the first `desired_sample_size` samples
            dataset = Subset(dataset, subset_indices)

            sub_sampled = True

    if not sub_sampled:
        print(f'Len of dataset is {s_subsample_dataset_to_len}')

    print(f'Actual length of the dataset for eval is: {len(dataset)}')


    print('Initializing Dataloader')

    # Data Loader
    # THIS FIXES FREEZING ISSUE!
    data_loader_eval_filtered = DataLoader(
        dataset,
        shuffle=True,
        batch_size=s_batch_size,
        drop_last=True,
        num_workers=0, # EITHER THIS
        pin_memory=False, # OR THIS FIXES FREEZING
        # timeout=0,  # TODO: Potentially try this to see whether the freezing happens during batch loading
    )

    # Original Data Loader ---> THIS CAUSES FREEZING ISSUE
    # data_loader_eval_filtered = DataLoader(
    #     data_set_eval_filtered,
    #     shuffle=False,
    #     batch_size=s_batch_size,
    #     drop_last=True,
    #     num_workers=s_num_workers_data_loader,
    #     pin_memory=True,
    # )

    print('Initialising Callback')

    # Callbacks
    evaluate_baseline_callback = EvaluateBaselineCallback(
            linspace_binning_params,
            checkpoint_name,
            dataset_name,
            samples_have_padding,
            ckpt_settings,
    )

    if s_fss:
        fss_callback = FSSEvaluationCallback(
            scales=s_fss_scales,
            thresholds=s_fss_thresholds,
            linspace_binning_params=linspace_binning_params,
            checkpoint_name=checkpoint_name,
            dataset_name=dataset_name,
            settings=ckpt_settings,
        )

        callbacks = [evaluate_baseline_callback, fss_callback]
    else:
        callbacks = evaluate_baseline_callback


    print('Initializing Trainer')

    trainer = pl.Trainer(
        callbacks=callbacks,
    )

    print('Starting evaluation with trainer.predict')

    trainer.predict(
        model=model,
        dataloaders=data_loader_eval_filtered,
        return_predictions=False  # By default, lightning aggregates the output of all batches, disable this to prevent memory overflow
    )
